<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <title>AI Safety Seminar 25</title>

  <link rel="stylesheet" href="dist/reset.css">
  <link rel="stylesheet" href="dist/reveal.css">
  <link rel="stylesheet" href="dist/theme/black.css">
  <link rel="stylesheet" href="css/mystyle.css">

  <!-- Theme used for syntax highlighted code -->
  <link rel="stylesheet" href="plugin/highlight/monokai.css">
  <link rel="icon" type="image/x-icon" href="./img/amba-favicon.ico">
</head>

<body> 
  <div class="reveal">
    <div class="slides">
      <section data-markdown data-separator-notes="^Note:">
        <textarea data-template>
        <section class="title-slide-lol">
        
        ## Introduction to AI ~~Ethics~~ Safety 
        Gabriele Graffieti
        
        ---- 

        IEEE Student Branch Seminar <!-- .element: class="smaller grey italic" --> 

        June 5, 2025 <!-- .element: class="smaller grey italic" -->  

        You can find this slides at: <!-- .element: class="smaller grey italic" --> 
        <br>
        [https://ggraffieti.github.io/slides_aiethics_bio_ieee](https://ggraffieti.github.io/slides_aiethics_bio_ieee)

        Note: 
        Interacion and questions
        </section> 
        ---
        ## Who I am
        <div class="two-c-container" data-markdown>
          <div class="two-c-col-l">
            <img src="./img/mine.jpg" width="90%">
          </div>
          <div class="two-c-col-r" data-markdown>

          - Sr. Algorithm Engineer @ [Ambrella](https://www.ambarella.com)
            - Deep Learning & Computer Vision
          - Past Head of AI research @ [AI for People](https://www.aiforpeople.org)
          - Past researcher \& PhD student @ [Unibo](https://www.unibo.it/en/homepage)
            - Main research interests: Continual Learning, GenAI and AI Ethics
          </div>
        </div>
        ---
        <section data-background-iframe="https://www.ambarella.com"
          data-background-interactive>
        </section>
        ---
        ### Ambarella - Vislab
        - Research division of Ambarella on self-driving cars:
          - 80+ people only in Parma. <!-- .element: class="fragment" data-fragment-index="1" -->
          - ~1,000 worldwide (US, TW, IT, CN, DE, KR, ...). <!-- .element: class="fragment" data-fragment-index="2" -->
          - On self-driving cars (sofware), our competitors are Tesla, Wayve, Waymo, Uber, etc.  <!-- .element: class="fragment" data-fragment-index="3" -->
          - On self-driving cars (hardware), our competitors are Nvidia, Mobileye, etc.  <!-- .element: class="fragment" data-fragment-index="4" -->
        ---
        ### Ambarella - Vislab 
        - 1998 1,000 miles (2,000+ km) on Italian highways with autonomous steering. <!-- .element: class="fragment" data-fragment-index="1" -->
        - 2005-2007 DARPA grand urban challenge, 100% autonomous. <!-- .element: class="fragment" data-fragment-index="2" -->
        - 2010 VIAC: 15k+ km autonomous driving (Parma-Shanghai). <!-- .element: class="fragment" data-fragment-index="3" -->
        - 2013 PROUD: 13km in Parma fully autonomous (L4). <!-- .element: class="fragment" data-fragment-index="4" -->
        - 2015: Acquisition by Ambarella. <!-- .element: class="fragment" data-fragment-index="5" -->
        - 2020: Full autonomous driving demo @ CES 2020 Las Vegas. <!-- .element: class="fragment" data-fragment-index="6" -->
        - 2022-onwards: autonomous driving L4 in all environment with a single low power chip (no GPU, no high end CPU). <!-- .element: class="fragment" data-fragment-index="7" -->
        ---
        <section>
          <iframe width="1120" height="630" src="https://www.youtube.com/embed/x1glAcRP1TM?t=28" VQ=hd1080 frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </section>
        ---
        ### Ambarella - Vislab
        - What we do:
          - State-of-the-art research on autonomous driving. <!-- .element: class="fragment" data-fragment-index="1" -->
          - Only company in Italy (and one of the very few in Europe) to be allowed to test and drive in any road, at any time, with any traffic condition. <!-- .element: class="fragment" data-fragment-index="2" -->
          - Both DL-based and classical approach to vehicle control, sensing, vision, etc. <!-- .element: class="fragment" data-fragment-index="3" -->
          - Sensing only based on cameras (1 stereo + 5 mono / 2 mono) + radars. <!-- .element: class="fragment" data-fragment-index="4" -->
        ---
        ### Ambarella - Vislab
        - What we offer:
          - A unique international research environment in Italy. <!-- .element: class="fragment" data-fragment-index="1" -->
          - Ideas → development → deployment in T=0. <!-- .element: class="fragment" data-fragment-index="2" -->
          - Both industrial & academic research. <!-- .element: class="fragment" data-fragment-index="3" -->
          - (Very) competitive salary & lot of benefits. <!-- .element: class="fragment" data-fragment-index="4" -->
        - What we want? <!-- .element: class="fragment" data-fragment-index="5" -->
          - You! <!-- .element: class="fragment" data-fragment-index="6" -->
          - Opening for thesis, PhD, jobs! <!-- .element: class="fragment" data-fragment-index="7" -->
          - If interested contact me or <!-- .element: class="fragment" data-fragment-index="8" -->
            - [careers-it@ambarella.com](mailto:careers@ambarella.com) <!-- .element: class="fragment" data-fragment-index="8" -->
            - [enascimbeni@ambarella.com](mailto:enascimbeni@ambarella.com) <!-- .element: class="fragment" data-fragment-index="8" -->
        ---
        <img src="./img/cropped-AIforPeople-logo-full-2.png" height="180px">
        
        _Our mission is to learn, pose questions and take initiative on how AI technology can be used for the social good._
        ---
        ## Overview
        - Data Safety. <!-- .element: class="fragment" data-fragment-index="1" -->
          - Bias. <!-- .element: class="fragment" data-fragment-index="2" -->
        - Model Safety. <!-- .element: class="fragment" data-fragment-index="3" -->
          - Attacks on Data. <!-- .element: class="fragment" data-fragment-index="4" -->
          - Attacks on Models. <!-- .element: class="fragment" data-fragment-index="5" -->
        - Foundational Models and Their Use. <!-- .element: class="fragment" data-fragment-index="6" -->
        - An Introduction to Countermeasures. <!-- .element: class="fragment" data-fragment-index="7" -->
        ---
        <section>

          ## Why AI safety?
          <a href="https://colab.research.google.com/drive/11J0UQC42BCXhXl6pjj68m2a9oNQgqKWS?usp=sharing" target="_blank">Demo!</a>
        </section>
        <section>

          We ran a clinical trial on some cancer patients, and we collected data on those who benefited from the treatment.

          We want to develop an ML model that, given the data of a new patient, will predict whether the patient will benefit from the new treatment. <!-- .element: class="fragment" data-fragment-index="1" -->
        </section>
        <section>
          
          1. What model should I use?
              - Classification problem -> classifier? <!-- .element: class="fragment" data-fragment-index="1" -->
              -  Do we have all the information to trace a boundary between the two classes? <!-- .element: class="fragment" data-fragment-index="2" -->
              - <!-- .element: class="fragment" data-fragment-index="3" --> Is the information we have <i>complete</i>? 
              - <!-- .element: class="fragment" data-fragment-index="4" --> Can the use of a <i>classifier</i> hide some problems we may have with the data? 

          <aside class="notes">
            Classification problem -> classifier, but I don't have complete information!
            <br>
            Discriminative models have an intrisic bias inside them for having complete information (being able to trace a boundary between the data starting from input data). 
            <br/>
            I dont know if I can correctly classify data with the input features I have.
          </aside>
        </section>
        <section data-auto-animate>
          <img src="./img/points.svg" height="400">
        </section>
        <section data-auto-animate>
          <img src="./img/border.svg" height="400">

          Ideas? <!-- .element: class="fragment" data-fragment-index="1" -->
        </section>
        <section data-auto-animate>
          <img src="./img/distributions.svg" height="600">
        </section>
        <section data-auto-animate>
          <img src="./img/onlyone.svg" height="500">

          Examples of generative models? <!-- .element: class="fragment" data-fragment-index="1" -->
          <aside class="notes">
            Back to the demo
          </aside>
        </section>
        <section>
          
          1. What model should I use?
              - Classification problem -> classifier?
              - Do we have all the information to trace a boundary between the two classes?
              - Is the information we have <i>complete</i>?
          2. Is the data safe to use? <!-- .element: class="fragment" data-fragment-index="1" --> 
              - How, who, where the data is collected? <!-- .element: class="fragment" data-fragment-index="2" -->
              - Is the data biased, inaccurate, mislabeled, imbalanced, etc? <!-- .element: class="fragment" data-fragment-index="3" -->
              - Does the data contain private information? <!-- .element: class="fragment" data-fragment-index="4" -->
              - Is the data physically safe? <!-- .element: class="fragment" data-fragment-index="5" --> 
                - Is the data obtained from trusted sources? <!-- .element: class="fragment" data-fragment-index="6" -->
                - Is the data stored safely? Is the data protected from unathorized alteration? <!-- .element: class="fragment" data-fragment-index="7" -->
        </section>
        ---
        ## Data safety - Bias
        ---
        ### Biases in the real world
        <img src="./img/medical_care.png"  height="300"> <!-- .element: class="fragment" data-fragment-index="1" -->
        <p class="fragment" data-fragment-index="1">Hint: <span class="fragment custom blur">think about how US healthcare works.</span></p>
        ---
        ### Biases in the real world
        <img src="./img/amazon_women.png"  height="300">
        <p>Hint: <span class="fragment custom blur">think about gender representation inside tech jobs.</span></p>
        ---
        ### Biases in the real world
        <img src="./img/biasUK.png"  height="300">
        
        Hurt first, fix later → typical of computer science. <br/> Physical vs. digital, if buggy make a new release, a new patch, etc. <!-- .element: class="fragment" data-fragment-index="1" -->
        ---
        ## Well, can we fix this right?
        ### How? <!-- .element: class="fragment" data-fragment-index="1" -->
        ---
        ### First of all we need to detect the problem!
        - How we tested the model?  <!-- .element: class="fragment" data-fragment-index="1" -->
          - Did train/validation/test sets were collected from the same distribution of data? <!-- .element: class="fragment" data-fragment-index="2" -->
          - What metrics we used to evaluate the performance? <!-- .element: class="fragment" data-fragment-index="3" -->
          - What we mean by performance? <!-- .element: class="fragment" data-fragment-index="4" -->

        Note: 
        Example of bad calibrated cameras in self-driving cars.
        ---
        ### But if we remove all gender, ethnicity, or unwanted information from the data?
        - The AI system can infer them from remaining information! <!-- .element: class="fragment" data-fragment-index="1" -->
          - Gender from height/weight ratio. <!-- .element: class="fragment" data-fragment-index="2" -->
          - Ethnicity from specific disorders.  <!-- .element: class="fragment" data-fragment-index="3" -->
          - Level of weath from geographical information. <!-- .element: class="fragment" data-fragment-index="4" -->
          - ... <!-- .element: class="fragment" data-fragment-index="5" -->

        Note: 
        Beware of correlation between data and over representation! 

        A dataset for type 2 diabetes is maily composed of overweight people 

        Models are LAZY
        ---
        ### But are models really that powerful?
        #### Spoiler: yes <!-- .element: class="fragment" data-fragment-index="1" -->
        ---
        <div class="r-stack">
        <img class="fragment fade-out" data-fragment-index="1" src="./img/random_gpt_paper.jpg" width="70%"> 
        
        <div class="two-c-container">
          <div class="two-c-col">
            <img class="fragment fade-in" data-fragment-index="1" src="./img/human_random.jpg"> 
          </div>
          <div class="two-c-col">
            <img class="fragment fade-in" data-fragment-index="2" src="./img/random_gpt.jpeg">
          </div>
        </div>
        </div>

        [Can LLMs Generate Random Numbers? Evaluating LLM Sampling in Controlled Domains](https://arxiv.org/abs/2403.00742) <!-- .element: class="fragment smaller fade-out" data-fragment-index="1" -->
        ---
        #### Who put all those biases into the training data? 
        ## You! <!-- .element: class="fragment" data-fragment-index="1" -->
        ---
        ### Biases are in everyday life!
        <div class="two-c-container" data-markdown>
          <div class="two-c-col-l">

          - Beauty bias <!-- .element: class="fragment" data-fragment-index="1" -->
          - Halo/Horns effect <!-- .element: class="fragment" data-fragment-index="2" -->
          - Conformity bias <!-- .element: class="fragment" data-fragment-index="3" --> 
          - Status quo bias <!-- .element: class="fragment" data-fragment-index="4" -->
          - Authority bias <!-- .element: class="fragment" data-fragment-index="5" -->
          - Idiosyncratic bias <!-- .element: class="fragment" data-fragment-index="6" -->
          - ... <!-- .element: class="fragment" data-fragment-index="7" -->
          </div>
          <div class="two-c-col-r" data-markdown>
            <img src="./img/women.png"> <!-- .element: class="fragment" data-fragment-index="8" -->
          </div>
        </div>
        Note:
        Idiosyncratic bias: occurs when managers evaluate skills they're not good at highly. Conversely, they rate others lower for skills they're great at. In other words, managers weigh their performance evaluations toward their own personal eccentricities.
        ---
        ### Biases are in everyday life!
        <img src="./img/face.jpg" height="300">
        ---
        <!-- .slide: data-auto-animate -->
        ### How to deal with bias
        - <!-- .element: class="fragment" data-fragment-index="1" --> <a href="https://www.forbes.com/councils/forbestechcouncil/2025/03/13/the-impossible-dream-why-bias-free-ai-is-a-myth">The Impossible Dream: Why Bias-Free AI Is A Myth</a>
        - Just thinking of and documenting the possible biases that AI models may have is a great step forward (we will discuss this later). <!-- .element: class="fragment" data-fragment-index="2" -->
        - Concretely: <!-- .element: class="fragment" data-fragment-index="3" -->
        ---
        <!-- .slide: data-auto-animate data-transition="slide-in fade-out" -->
        ### How to deal with bias
        - Concretely:
          - Labeled data: <!-- .element: class="fragment" data-fragment-index="1" -->
            - Balance unbalanced datasets. <!-- .element: class="fragment" data-fragment-index="2" -->
            - Remove data that contribute (most) to the bias.  <!-- .element: class="fragment" data-fragment-index="3" -->  
              [Improving Subgroup Robustness via Data Selection](https://proceedings.neurips.cc/paper_files/paper/2024/hash/abbbb25cddb2c2cd08714e6bfa2f0634-Abstract-Conference.html) <!-- .element: class="fragment smaller" data-fragment-index="3" -->
          - Unlabeled data: <!-- .element: class="fragment" data-fragment-index="4" -->
            - Adversarial debiasing.  <!-- .element: class="fragment" data-fragment-index="5" -->  
              [Mitigating Unwanted Biases with Adversarial Learning](https://dl.acm.org/doi/10.1145/3278721.3278779) <!-- .element: class="fragment smaller" data-fragment-index="5" -->
            - Computing attention to detect biases.  <!-- .element: class="fragment" data-fragment-index="6" -->  
              [Visualizing and Understanding Convolutional Networks](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) <!-- .element: class="fragment smaller" data-fragment-index="6" -->
          - And semi-supervised learining / pseudo-labels?  <!-- .element: class="fragment" data-fragment-index="7" -->
        ---
        [Demo](https://colab.research.google.com/drive/1nSIbchuITfdvlbjF-4R9aerTdsjHa5CV)
        ---
        <!-- .slide: data-auto-animate data-transition="zoom-in slide-out" -->
        Questions? <!-- .element: class="bigger" --> 

        Discussion? <!-- .element: class="bigger" --> 
        ---
        ## Data safety - Attacks
        ---
        ### Attacks on data
        - Even if data is formally correct, it can be attacked by (malicious) third parties in different ways:
          - Adversarial attacks (real-world or hidden) <!-- .element: class="fragment" data-fragment-index="1" -->
          - Poisoning  <!-- .element: class="fragment" data-fragment-index="2" -->
          - Backdoor insertion <!-- .element: class="fragment" data-fragment-index="3" -->
          - Mislabeling <!-- .element: class="fragment" data-fragment-index="4" -->
          - Deliberate model fail  <!-- .element: class="fragment" data-fragment-index="5" -->
          - ... <!-- .element: class="fragment" data-fragment-index="6" -->
        - Common goal: making the resulting model unusable, weak, attackable or exploitable.  <!-- .element: class="fragment" data-fragment-index="7" -->
        ---
        ### Adversarial examples - real world
        <img src="img/signs2.webp">

        [Robust Physical-World Attacks on Deep Learning Visual Classification](https://ieeexplore.ieee.org/document/8578273)  <!-- .element: class="smaller" -->
        ---
        ### Adversarial examples - hidden
        <img src="img/cat_guacamole.png">

        [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf)  <!-- .element: class="smaller" -->
        ---
        <img src="./img/adv_ex3.png" height="400px">

        [Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation](https://link.springer.com/chapter/10.1007/978-3-030-32245-8_34)  <!-- .element: class="smaller" -->

        Note:
        Think about controlling the input pipeline of images (attack on the hospital system).
        ---
        ### Backdoor Insertion
        - Backdoor → insertion of a weakness in a system, exploited to enter or control the system. <!-- .element: class="fragment" data-fragment-index="1" -->
        - The training data is altered in order to insert a backdoor in the model. <!-- .element: class="fragment" data-fragment-index="2" -->
        - Once the backdoor is inserted, it can be exploited to control the output of the model.  <!-- .element: class="fragment" data-fragment-index="3" -->
        - The model works normally if regular data is used. <!-- .element: class="fragment" data-fragment-index="4" -->
        
        <br/>
        <br/>

        [Demo!](https://colab.research.google.com/drive/1co74MX84NLvBlu-X79GsyvSk7sMEJg2D?usp=sharing) <!-- .element: class="fragment" data-fragment-index="4" -->
        ---
        ### Backdoors in Federated Learning

        <img src="img/federated_learning.png" >

        [How To Backdoor Federated Learning](https://proceedings.mlr.press/v108/bagdasaryan20a.html) <!-- .element: class="fragment smaller" data-fragment-index="1" -->
        ---
        ## Model Safety - Attacks
        ---
        ### AI models under attack
        - Even if the data and the resulting AI model are safe, the former can be attacked by (malicious) third parties in different ways:
          - (Malicious) Prompt engineering <!-- .element: class="fragment" data-fragment-index="1" -->
          - Denial of Service  <!-- .element: class="fragment" data-fragment-index="2" -->
          - Waste or overuse of resources <!-- .element: class="fragment" data-fragment-index="3" -->
          - Model inversion & membership attack <!-- .element: class="fragment" data-fragment-index="4" -->
          - ... <!-- .element: class="fragment" data-fragment-index="5" -->
        - Common goal: fooling the model into yielding wrong outputs, (mis)using the model beyond its design, extract hidden information.  <!-- .element: class="fragment" data-fragment-index="6" -->
        ---
        ### (Malicious) Prompt Engineering
        - Prompt engineering → process of carefully crafting input instructions to maximize the effectiveness and relevance of outputs of genAI models.  <!-- .element: class="fragment" data-fragment-index="1" -->
        - Malicious prompt engineering: <!-- .element: class="fragment" data-fragment-index="2" -->
          - Using specific prompts or inserting token into regular prompt to: <!-- .element: class="fragment" data-fragment-index="3" -->
            - Bypass ethical filters of the model. <!-- .element: class="fragment" data-fragment-index="4" -->
            - Force the model to yield incorrect or wrong information. <!-- .element: class="fragment" data-fragment-index="5" -->
            - Pollute the context of the model, impacting every successive regular prompt.  <!-- .element: class="fragment" data-fragment-index="6" -->
        ---
        ### (Malicious) Prompt Engineering

        <img src="img/chatGPT_wrong.png" height="400px">
        <img src="img/chatGPT_wrong2.png" height="400px">

        [Prompt Injection Attack on GPT-4](https://www.robustintelligence.com/blog-posts/prompt-injection-attack-on-gpt-4) <!-- .element: class="smaller" -->
        ---
        ### Membership attack
        <img src="./img/gptprivacy.png" height="150">
        <img src="./img/extractingdata.png" class="fragment" data-fragment-index="1" height="400">

        Note: 
        Why be worried by chatGPT trained with private data? 

        Because it can be reconstructed!
        ---
        ## How to Build Safe AI
        ---
        ### The long and winding road
        - Buld a 100% safe AI is impossible and beyound our control. <!-- .element: class="fragment" data-fragment-index="1" -->
          - Biases hidden in the data, bad labeling, etc. <!-- .element: class="fragment" data-fragment-index="2" -->
          - Security flaws in HHDs where the data is stored, libraries/frameworks used, etc. <!-- .element: class="fragment" data-fragment-index="3" -->
          - Adversarial examples, model inversion, flawed third party AIs, etc. <!-- .element: class="fragment" data-fragment-index="4" -->
        - But we can do as better as we can!  <!-- .element: class="fragment" data-fragment-index="5" -->
          - Following guidelines to build safe AIs in the particular domain.   <!-- .element: class="fragment" data-fragment-index="6" -->
          - Have a critical and objective view of the possible problems of the AI you are building.  <!-- .element: class="fragment" data-fragment-index="7" -->
          - Mind that every phase of AI development is important to the overall safety! <!-- .element: class="fragment" data-fragment-index="8" -->
            - From design to data acqusition, to development to end of life! <!-- .element: class="fragment" data-fragment-index="9" -->
        ---
        ### How to Improve Safety of AIs
        - <!-- .element: class="fragment" data-fragment-index="1" --> If it's possible use explainable AI models <span class="fragment" data-fragment-index="2">(spoiler: often not possible).</span> 
        - <!-- .element: class="fragment" data-fragment-index="3" --> Use at least 2 <u>totally different</u> AI models in safety critical applications. 
          - Different data used to train them, different model architectures, different outputs, etc.  <!-- .element: class="fragment" data-fragment-index="4" -->
          - Validate the output of each of them with one another.   <!-- .element: class="fragment" data-fragment-index="5" -->
        - Simulate before deploying.  <!-- .element: class="fragment" data-fragment-index="6" -->
          - Use simulations or simulated data to test the behavior of the model, especially in corner cases.  <!-- .element: class="fragment" data-fragment-index="7" -->
          - If possile use human actions/decision to calibrate the model.   <!-- .element: class="fragment" data-fragment-index="8" -->
        - Use adversarial models to find out flaws, biases and backdoors in the model.  <!-- .element: class="fragment" data-fragment-index="9" -->
        - Explore model attention to understand possible biases.  <!-- .element: class="fragment" data-fragment-index="10" -->

        Note: 
        2 models -> example amba-aur

        simulation -> self driving car system active but without control, to check with the driver decision real time. 
        ---
        ### How to Improve Safety of AIS - ISO/PAS 8800
        - Standard ISO that regulates "Road vehicles — Safety and Artificial Intelligence".  <!-- .element: class="fragment" data-fragment-index="1" -->
        - Published in Dec. 2024. <!-- .element: class="fragment" data-fragment-index="2" -->
        - Covers all the use of AI in road vehicles, to driving assistant technologies do ADS (another ISO standard is currently under publication for ADS only). <!-- .element: class="fragment" data-fragment-index="3" -->
        - Many concepts can be applyied 1:1 to any safety critical domain. <!-- .element: class="fragment" data-fragment-index="4" -->
        - Full of references to other normatives/ISO standars, from SW/HW testing to IT infrastructure and cybersecurity. <!-- .element: class="fragment" data-fragment-index="5" -->
        - No mention of possible attacks to AI models. <!-- .element: class="fragment" data-fragment-index="6" -->
        ---
        ### ISO/PAS 8800
        - For being certified ISO/PAS 8800 compliant need to satisfy all the safety requirements (or at least have a strong justification if some requirement is not satisfied). <!-- .element: class="fragment" data-fragment-index="1" -->
        - But same guidelines may greatly improve all the AI ecosystem! <!-- .element: class="fragment" data-fragment-index="2" -->
          - There is no shame in a (possibly) unsafe AI system!  <!-- .element: class="fragment" data-fragment-index="3" -->
          - <!-- .element: class="fragment" data-fragment-index="4" --> Just attach all the documentation and <u>clearly state</u> how the system may fail and in which occasions.
          - Also include untested scenarios (e.g. I only tested my ADS in right-hand drive countries). <!-- .element: class="fragment" data-fragment-index="5" -->
          - Minimize the risk of misuse of AI systems by third parties. <!-- .element: class="fragment" data-fragment-index="6" -->
            - Will you use an online AI model to evaluate CVs if it is clearly stated that it may contain biases against women, blacks, minorities, etc.? <!-- .element: class="fragment" data-fragment-index="7" -->
        ---
        <section>

          ## Bonus: AI Democratization
        </section>
        <section>

          ### Who owns AI?
          - AI needs (a big) infrastructure: <!-- .element: class="fragment" data-fragment-index="1" -->
            - The algorithm is just a small part of the product. <!-- .element: class="fragment" data-fragment-index="2" -->
            - Computational capabilities (computational power and memory) are fundamental. <!-- .element: class="fragment" data-fragment-index="3" -->
            - Only the biggest companies have the workforce to maintain a solid infrastructure. <!-- .element: class="fragment" data-fragment-index="4" -->
              - Substantial advantage over smaller companies or academia. <!-- .element: class="fragment" data-fragment-index="5" -->
        </section>
        <section>

          ### Who owns AI?
          - AI needs (a lot of) data:
            - Data is essential to reproduce results. <!-- .element: class="fragment" data-fragment-index="1" -->
            - Data is often more important than algorithm (who owns data?). <!-- .element: class="fragment" data-fragment-index="2" -->
            - Big tech companies have the possibility to own or acquire a huge amount of data. <!-- .element: class="fragment" data-fragment-index="3" -->
              - Substantial advantage over smaller companies or academia. <!-- .element: class="fragment" data-fragment-index="4" -->
        </section>
        <section>
        
        ### The myth of AI Democratization
        - AI big companies claim to be democratic:
          - Sharing their research (e.g. arXiv). <!-- .element: class="fragment" data-fragment-index="1" -->
          - Sharing their code (e.g. github). <!-- .element: class="fragment" data-fragment-index="2" -->
          - Sharing their frameworks (e.g. Tensorflow). <!-- .element: class="fragment" data-fragment-index="3" -->
          - Sharing their infrastructure (?) (e.g. colab). <!-- .element: class="fragment" data-fragment-index="4" -->
        
        >  <!-- .element: class="fragment" data-fragment-index="5" --> [...] at an increasing scale, consumers have greater access to use and purchase technologically sophisticated products, <span style="color:#44AFF6">as well as to participate meaningfully in the development of these products.
        </section>
        <section>
        
          ### The myth of AI Democratization
          <img src="./img/ceobiden.jpg" height="500px">

          White House meeting on the threat of AI - May 5, 2023  <!-- .element: class="smaller" -->
        </section>
        <section>

          <img src="./img/altman1.png" height="200px">

          The Guardian - May 16, 2023 <!-- .element: class="smaller" -->

          <img class="fragment" data-fragment-index="1" src="./img/altman2.png" height="200px">

          The Verge - May 25, 2023 <!-- .element: class="smaller fragment" data-fragment-index="1" -->
        </section>
        <section>
        
          ### The myth of AI Democratization
          - AI is currently owned by few companies
            - They have access to a huge amount of data. <!-- .element: class="fragment" data-fragment-index="1" -->
            - They attract top AI scientists (huge salaries, freedom). <!-- .element: class="fragment" data-fragment-index="2" -->
            - They have the power to transform research ideas into products. <!-- .element: class="fragment" data-fragment-index="3" -->
        </section>
        <section>

          ### Why a democratic AI is important
          - Avoid monopolies. <!-- .element: class="fragment" data-fragment-index="1" -->
          - <!-- .element: class="fragment" data-fragment-index="2" --> Democratization means that everyone gets the <span style="color:#44AFF6">opportunities and benefits of artificial intelligence.</span> 
          - Openness in AI development is proved to be beneficial to the development of better technologies. <!-- .element: class="fragment" data-fragment-index="3" -->
        </section>
        <section>

          ### AI ownership
          - Who owns the outputs produced by genAI? Are outputs of AI models copyrightable? <!-- .element: class="fragment" data-fragment-index="1" -->

          <img src="./img/aiart.png" height="150"> <!-- .element: class="fragment" data-fragment-index="2" -->

          - What about code? <!-- .element: class="fragment" data-fragment-index="3" -->
          - What kind of license applies to ChatGPT generated code is still not clear. <!-- .element: class="fragment" data-fragment-index="4" -->
          - Legally, the implications of using ChatGPT generated code in commercial product are still unknown. <!-- .element: class="fragment" data-fragment-index="5" -->
        </section>
        ---
        <section>

          ## Bonus: AI and Climate Change 
        </section>
        <section>

          ### Climate change
          - What is the carbon footprint of training a large AI model?
            - <!-- .element: class="fragment" data-fragment-index="1" --> A <a href="https://arxiv.org/pdf/2104.10350">2021 paper</a> from Google estimates that a single training of GPT-3 emits ~552,000kg of CO$_2$.
              - GPT-4 is estimated to be more than 4x the size of GPT-3. <!-- .element: class="fragment" data-fragment-index="2" -->
              - We takes into account HW improvement, so a 2x multiplier is applied → ~1,104,000kg of CO$_2$.  <!-- .element: class="fragment" data-fragment-index="3" -->
              - GPT-4 is trained for more than 100 days, while GPT-3 only for 14 days, so another 7x multiplier is applied → 7,728,000kg of CO$_2$.  <!-- .element: class="fragment" data-fragment-index="4" -->
        </section>
        <section>

          ### Climate change
          - But this is only one single training!
            - Let's assume at least 100 explorative training averaging 0.2x the last one. <!-- .element: class="fragment" data-fragment-index="1" -->
            - 7,728,000 + (7,728,000 $\cdot$ 0.2 $\cdot$ 100) = 162,288,000kg of CO$_2$. <!-- .element: class="fragment" data-fragment-index="2" -->
          - And what about deployment? <!-- .element: class="fragment" data-fragment-index="3" -->
            - OpenAI claims to constantly run 30,000 NVidia A100 GPUs, 300W each. <!-- .element: class="fragment" data-fragment-index="4" -->
            - <!-- .element: class="fragment" data-fragment-index="5" --> Suppose 70% average utilization for one year → 55,188,000kWh of energy, equal to 21,718,000kg of CO$_2$ emission (<a href="https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator">source</a>).
          - Total = 184,006,000kg of emitted CO$_2$. <!-- .element: class="fragment" data-fragment-index="6" -->
        </section>
        <section>

          ### Climate change
          - To make that into perspective: 184,006,000kg of CO<sub>2</sub> emission are the same as:
            - 42,920 gasoline-powered passenger vehicles driven for one year.  <!-- .element: class="fragment" data-fragment-index="1" -->
            - 162,505 electric-powered passenger vehicles driven for one year.  <!-- .element: class="fragment" data-fragment-index="2" -->
            - 92,712,741kg of coal burned. <!-- .element: class="fragment" data-fragment-index="3" -->
            - 38,346 homes' electricity use for one year. <!-- .element: class="fragment" data-fragment-index="4" -->
            - 55 wind turbines running for a year. <!-- .element: class="fragment" data-fragment-index="5" -->
            - Carbon sequestred from 747km$^2$ of US forests. <!-- .element: class="fragment" data-fragment-index="6" -->
        </section>
        <section>

          ### Climate change
          <img src="./img/carbonmap.png" height="400">
        </section>
        <section>

          ### Climate change
          - But here we only calculated training and deployment stages, the full carbon emission of a ML model is composed of [all these phases](https://www.sciencedirect.com/science/article/pii/S2095809924002315): 
            - ML R&D <!-- .element: class="fragment" data-fragment-index="1" -->
            - Hardware manufacturing (material extraction?) <!-- .element: class="fragment" data-fragment-index="2" -->
            - Global commercial logistics <!-- .element: class="fragment" data-fragment-index="3" -->
            - Facility O&M <!-- .element: class="fragment" data-fragment-index="4" -->
            - Massive data collection and management <!-- .element: class="fragment" data-fragment-index="5" -->
            - Model training and fine-tuning <!-- .element: class="fragment" data-fragment-index="6" -->
            - Deployment stage <!-- .element: class="fragment" data-fragment-index="7" -->
            - Material recycling and waste disposal <!-- .element: class="fragment" data-fragment-index="8" -->
        </section>
        <section>

          ### Climate change 
          - But AI and ML are (fortunately) far for being a leading cause for climate change
            - <!-- .element: class="fragment" data-fragment-index="1" --> In a month commercial airplanes emit at least 65x10$^9$kg of CO$_2$ (<a href="https://ourworldindata.org/grapher/monthly-co2-emissions-from-international-and-domestic-flights?time=2022-10-15..latest">source</a>).
            - Many companies claims to use high percentages of renewable energy for their data centers. <!-- .element: class="fragment" data-fragment-index="2" -->
            - But... <!-- .element: class="fragment" data-fragment-index="3" -->
          
          <img class="fragment" data-fragment-index="4" src="./img/googleenergy.png" height="100">

          [The growing energy footprint of artificial intelligence](https://www.cell.com/joule/abstract/S2542-4351(23)00365-3) <!-- .element: class="fragment smaller" data-fragment-index="4" -->
        </section>
        <section>

          ### Climate change
          <img src="./img/gptcc.png" height="500">
        </section>
        ---
        <!-- .slide: data-transition="slide-in fade-out" -->
        ### To summarize
        - The widespread adoption of AI in high risk applications pose significant safety risks. <!-- .element: class="fragment" data-fragment-index="1" -->
        - AI models may (and will) have unexpected bad outcomes.  <!-- .element: class="fragment" data-fragment-index="2" -->
          - Bias on the data/model is a major enemy. <!-- .element: class="fragment" data-fragment-index="3" -->
          - It is often impossible to predict a priori all the possible bad outcomes. <!-- .element: class="fragment" data-fragment-index="4" -->
        - Malicious individuals may (and will) attack AI models and the data they are trained on. <!-- .element: class="fragment" data-fragment-index="5" -->
        ---
        <!-- .slide: data-auto-animate -->
        ### My personal advice
        - Always think about the possible safety problems of your AI system. <!-- .element: class="fragment" data-fragment-index="1" -->
          - From data aquisition to deployment (and beyond!). <!-- .element: class="fragment" data-fragment-index="2" -->
        - Invest time analyzing training data, how and who acquired it, how it is stored, possible biases, etc. <!-- .element: class="fragment" data-fragment-index="3" -->
        - Analyze possible attack scenarios of your models, minding that if it's attackable, it will be attacked. <!-- .element: class="fragment" data-fragment-index="4" -->
        - Do not fall for easy and fast enthusiasm: the possible bad outcomes are often hidden and difficult to spot. <!-- .element: class="fragment" data-fragment-index="5" -->
        - On the other hand, try to not become resistant, anti, or too critic of AI.  <!-- .element: class="fragment" data-fragment-index="6" -->
        ---
        <!-- .slide: data-auto-animate -->
        <img src="./img/thatsall_white.png" height="300px">
        ---
        <!-- .slide: data-auto-animate" -->
        Questions? <!-- .element: class="bigger" --> 

        Discussion? <!-- .element: class="bigger" --> 
        ---
        <!-- .slide: class="title-slide-lol" -->
        ## Introduction to AI ~~Ethics~~ Safety 
        Gabriele Graffieti
        
        ---- 

        You can find this slides at: <!-- .element: class="smaller grey italic" --> 
        <br>
        [https://ggraffieti.github.io/slides_aiethics_bio_ieee](https://ggraffieti.github.io/slides_aiethics_bio_ieee)
      </textarea>
      </section>
    </div>
  </div>
  <script src="dist/reveal.js"></script>
  <script src="plugin/notes/notes.js"></script>
  <script src="plugin/markdown/markdown.js"></script>
  <script src="plugin/highlight/highlight.js"></script>
  <script src="plugin/math/math.js"></script>
  <script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
      hash: true,
      progress: false,
      slideNumber: 'c',
      
      // Learn about plugins: https://revealjs.com/plugins/
      plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX]
    });
  </script>
</body>

</html>
